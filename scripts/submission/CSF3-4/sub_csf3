#!/bin/python3
import os
import sys
import argparse

# create parser for parsing input file name
parser = argparse.ArgumentParser(description='Create Orca slurm runscript')
parser.add_argument("-i", "--input", help="Input file name", required=True, type=str)
parser.add_argument("-n", "--nodes", help="Number of nodes. multicore=1, multinode>1, serial=0; default=1", default=1, type=int)
parser.add_argument("-c", "--cores", help="Number of cores. default=8", default=8, type=int)
parser.add_argument("-l", "--queue", help="Queue type: High Memory = mem512 (Max 16 cores, 32GB per), mem256 (Max 16 cores, 16GB per). Short (1h) = short (Max 24 cores, 4GB per)", type=str, choices=['mem512', 'mem256', 'short'])
args = parser.parse_args()

# check if number of nodes is valid

if args.cores > 32 and args.nodes < 2:
    print("Number of cores must be between 2 and 32")
    sys.exit()
elif args.cores == 1:
    cores_info = "#"
elif args.cores > 1 and args.cores <= 32:
    cores_info = "#$ -pe smp.pe"

# check if queue type is valid

# check if queue type is valid
if args.queue == 'mem512' or args.queue == 'mem256':
    if args.cores > 16:
        print("Number of cores for high memory CPUs must be between 16 or less")
        sys.exit()
elif args.queue == 'short':
    if args.cores > 24:
        print("Number of cores for short queue must be 24 or less")
        sys.exit()

# check if number of cores is the same as in input file
#def check_cores(input_file, cores):
#    with open(input_file, "r") as f:
#        lines = f.readlines()
#        for line in lines:
#            if "nprocs" in line:
#                if int(line.split()[2]) != cores:
#                    print("Number of cores in input file does not match number of cores specified")
##                    ask user if they want to change number of cores in input file
#                    change_cores = input("Do you want to change the number of cores in the input file? (y/n)")
#                    if change_cores == "y":
#                        with open(input_file, "w") as f:
#                            for line in lines:
#                                if "nproc" in line:
#                                    line = f"%pal   nprocs {cores}\n"
#                                f.write(line)
                    # else:
                        # sys.exit()

# Check for program(s)
with open(args.input, "r") as f:
    lines = f.readlines()
    for line in lines:
         if "nbo" in line.lower():
            program = "nbo"
            break
         else:
            program = ""

# create slurm runscript (g16)
if args.input[-4:] == '.com' and program == '':
    def create_runscript(cores_info, cores, input_file):
        script = f"""#!/bin/bash --login

#####SGE Parameters
#$ -cwd                 # Run job in directory you submitted from
{cores_info} {cores}         # Number of cores (2--32) on single compute node
#$ -N {input_file.split(".")[0]}      # Job name """ + """

# Load g16 for the CPU type our job is running on
module load apps/binapps/gaussian/g16c01_em64t_detectcpu

## Set up scratch dir (please do this!)
export GAUSS_SCRDIR=/scratch/$USER/gau_temp_$JOB_ID
mkdir -p $GAUSS_SCRDIR

## Say how much memory to use (5GB per core)
export GAUSS_MDEF=$((NSLOTS*4))GB

## Say how many threads
export OMP_NUM_THREADS=$NSLOTS

## Inform Gaussian how many cores to use
export GAUSS_PDEF=$NSLOTS
$g16root/g16/g16 < ${JOB_NAME}.com > ${JOB_NAME}.log
rm -rf $GAUSS_SCRDIR"""
        return script

# create slurm runscript (g16) with nbo7
elif args.input[-4:] == '.com' and program == 'nbo':
    def create_runscript(cores_info, cores, input_file):
        script = f"""#!/bin/bash --login

#####SGE Parameters
#$ -cwd                 # Run job in directory you submitted from
{cores_info} {cores}         # Number of cores (2--32) on single compute node
#$ -N {input_file.split(".")[0]}      # Job name """ + """

# Load g16 for the CPU type our job is running on
module load apps/binapps/gaussian/g16c01_em64t_detectcpu

# Load NBO 7.0.8
module load apps/binapps/nbo/7.0.8

## Set up scratch dir (please do this!)
export GAUSS_SCRDIR=/scratch/$USER/gau_temp_$JOB_ID
mkdir -p $GAUSS_SCRDIR

## Say how much memory to use (5GB per core)
export GAUSS_MDEF=$((NSLOTS*4))GB

## Say how many threads
export OMP_NUM_THREADS=$NSLOTS

## Inform Gaussian how many cores to use
export GAUSS_PDEF=$NSLOTS
$g16root/g16/g16 < ${JOB_NAME}.com > ${JOB_NAME}.log
rm -rf $GAUSS_SCRDIR"""
        return script

# create slurm runscript (ORCA)
elif args.input[-4:] == '.inp':
    def create_runscript(cores_info, cores, input_file):
        script = f"""#!/bin/bash --login
#$ -cwd
{cores_info} {cores}          # Number of cores (2--32). Must match in your ORCA input file!
#$ -N {input_file.split(".")[0]}      # Job name""" + """
module purge
module load apps/binapps/orca/5.0.4

##########################
job=${JOB_NAME}

job=$(echo ${job%%.*})
##########################
export scratchlocation=/scratch

if [ ! -d $scratchlocation/$USER ]

then

mkdir -p $scratchlocation/$USER

fi
##########################
tdir=$(mktemp -d $scratchlocation/$USER/orcajob_$JOB_ID-XXXX)

# Copy only the necessary stuff in submit directory to scratch directory. Add more here if needed.

cp  $SGE_O_WORKDIR/*.inp $tdir/

cp  $SGE_O_WORKDIR/*.gbw $tdir/

cp  $SGE_O_WORKDIR/*.xyz $tdir/
#########################

# Creating nodefile in scratch

# echo $HOSTNAME > $tdir/$job.nodes

# cd to scratch

cd $tdir

# Copy job and node info to beginning of outputfile

echo "Job execution start: $(date)" >>  $SGE_O_WORKDIR/$job.out

echo "Shared library path: $LD_LIBRARY_PATH" >>  $SGE_O_WORKDIR/$job.out

echo "Slurm Job ID is: ${JOB_ID}" >>  $SGE_O_WORKDIR/$job.out

echo "Slurm Job name is: ${JOB_NAME}" >>  $SGE_O_WORKDIR/$job.out

echo $HOSTNAME >> $SGE_O_WORKDIR/$job.out

#Start ORCA job. ORCA is started using full pathname (necessary for parallel execution). Output file is written directly to submit directory on frontnode.

$(which orca) $job.inp >  $SGE_O_WORKDIR/$job.out

# ORCA has finished here. Now copy important stuff back (xyz files, GBW files etc.). Add more here if needed.

cp $tdir/*.gbw $SGE_O_WORKDIR

cp $tdir/*.engrad $SGE_O_WORKDIR

cp $tdir/*.xyz $SGE_O_WORKDIR

cp $tdir/*.loc $SGE_O_WORKDIR

cp $tdir/*.qro $SGE_O_WORKDIR

cp $tdir/*.uno $SGE_O_WORKDIR

cp $tdir/*.unso $SGE_O_WORKDIR

cp $tdir/*.uco $SGE_O_WORKDIR

cp $tdir/*.hess $SGE_O_WORKDIR

cp $tdir/*.cis $SGE_O_WORKDIR

cp $tdir/*.dat $SGE_O_WORKDIR

cp $tdir/*.mp2nat $SGE_O_WORKDIR

cp $tdir/*.nat $SGE_O_WORKDIR

cp $tdir/*.scfp_fod $SGE_O_WORKDIR

cp $tdir/*.scfp $SGE_O_WORKDIR

cp $tdir/*.scfr $SGE_O_WORKDIR

cp $tdir/*.nbo $SGE_O_WORKDIR

cp $tdir/FILE.47 $SGE_O_WORKDIR

cp $tdir/*_property.txt $SGE_O_WORKDIR

cp $tdir/*spin* $SGE_O_WORKDIR
"""
        return script

# check if input file exists
if not os.path.isfile(args.input):
    print("Input file does not exist")
    sys.exit()
# if input file exists, create slurm runscript and adjust number of cores
else:
    # check if number of cores in input file matches number of cores specified
    with open("run.sh", "w") as f:
        f.write(create_runscript(cores_info, args.cores, args.input))
    if args.queue:
        os.system(f"qsub -l {args.queue} run.sh {args.input}")
    else:
        os.system(f"qsub run.sh {args.input}")

